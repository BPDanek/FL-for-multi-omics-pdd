{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first examine how the directories are set up, so you have a better idea how the GitHub repository is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\n",
      "README.md\n",
      "\u001b[34mdata\u001b[m\u001b[m\n",
      "environment.yml\n",
      "features.csv\n",
      "\u001b[34mfederated_learning_multi_modality_ancestry\u001b[m\u001b[m\n",
      "\u001b[34mfederated_learning_multi_modality_ancestry.egg-info\u001b[m\u001b[m\n",
      "\u001b[34mnotebooks\u001b[m\u001b[m\n",
      "\u001b[34mreports\u001b[m\u001b[m\n",
      "setup.py\n"
     ]
    }
   ],
   "source": [
    "! ls ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main GitHub directory has two files: 1) README.md that includes general information about the GitHub repository and 2) environment.yml that includes a list of Python packages needed. This file was used in the previous tutorial notebook to create a conda environemtn multi_omics_pdd_fl.\n",
    "\n",
    "This directory also includes two directories: 1) tutorial that includes tutorial notebooks and 2) federated_learning_multi_modality_ancestry that includes all the necessary directories & files to perform federated learning on PDD data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will walk through federated_learning_multi_modality_ancestry directory to see where all the files for running the experiment are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipfile           \u001b[34mcollab_saves\u001b[m\u001b[m      package-lock.json\n",
      "__init__.py       \u001b[34mmulti_modality_fl\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../../federated_learning_multi_modality_ancestry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before this, let's see where our data is stored in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mCombined_G1E5_O1E2\u001b[m\u001b[m \u001b[34mValidation\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directory dataincludes two directories (Combined_G1E5_O1E2 and Validation) with data files inside that our group used for the experiment and analysis in the preprint. As mentioned in the front page of GitHub repository, these files can be produced by reproducing the processing approach from \n",
    "```\n",
    "Makarious, M.B., Leonard, H.L., Vitale, D. et al. Multi-modality machine learning predicting Parkinsonâ€™s disease. npj Parkinsons Dis. 8, 35 (2022). https://doi.org/10.1038/s41531-022-00288-w\n",
    "```\n",
    "Available publicly at: https://github.com/GenoML/GenoML_multimodal_PD/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI-genetic_p1E5_omic_p1E2.dataForML.h5\n"
     ]
    }
   ],
   "source": [
    "! ls ../../data/Combined_G1E5_O1E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate-PDBP-genetic_p1E5_omic_p1E2.dataForML.h5\n"
     ]
    }
   ],
   "source": [
    "! ls ../../data/Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the directory multi_modality_fl includes all the Python files required to perform federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py       \u001b[34mexperiments\u001b[m\u001b[m       \u001b[34mnotebooks\u001b[m\u001b[m\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m       \u001b[34mexperiments_code\u001b[m\u001b[m  \u001b[34mresults\u001b[m\u001b[m\n",
      "\u001b[34mexperiment_runner\u001b[m\u001b[m \u001b[34mmodels\u001b[m\u001b[m            \u001b[34mutils\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../../federated_learning_multi_modality_ancestry/multi_modality_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_experiment_runs.py     run_experiments.py\n",
      "experiment_runner.py          run_single_experiment copy.py\n",
      "experiment_runner.sh          run_single_experiment.py\n",
      "run_baseline_experiments.py\n"
     ]
    }
   ],
   "source": [
    "! ls ../../federated_learning_multi_modality_ancestry/multi_modality_fl/experiment_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python file run_experiments.py is a wrapper file to run the experiment. If you examine this wrapper file using terminal or IDE of your choice, you can see that it imports from utils and experiments_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py                   data_splitting_strategy.ipynb\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                   hdf_to_csv.py\n",
      "data_management.py\n"
     ]
    }
   ],
   "source": [
    "! ls ../../federated_learning_multi_modality_ancestry/multi_modality_fl/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py                flwr_fed_tf.py\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                nvflare_fed_linear.py\n",
      "baseline.py                nvflare_fed_rfxgb.py\n",
      "flwr_fed_logreg.py         test_nvflare_fed_linear.py\n",
      "flwr_fed_mlp.py            test_nvflare_fed_rfxgb.py\n"
     ]
    }
   ],
   "source": [
    "! ls ../../federated_learning_multi_modality_ancestry/multi_modality_fl/experiments_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to examine the individual Python files in these directories further to learn more about how the code is set up. For now, we will go to the next tutorial and perform federated learning using the wrapper file run_experiments.py."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
