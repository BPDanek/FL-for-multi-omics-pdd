{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476c15bc",
   "metadata": {},
   "source": [
    "File computing the baseline performance of centralized algorithms from: https://www.nature.com/articles/s41531-022-00288-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15372da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ddb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_modality_fl.utils.data_management import BASE_PATH, DATASETS, build_full_path, drop_id, get_h5_data_keys, standardize_for_validation, normalize_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = build_full_path(BASE_PATH, DATASETS['combined_ppmi'])\n",
    "print(dataset_path)\n",
    "key = get_h5_data_keys(dataset_path)\n",
    "dataset_full = pd.read_hdf(dataset_path, key=key[0])\n",
    "display(drop_id(dataset_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = build_full_path(BASE_PATH, DATASETS['validation_pdbp'])\n",
    "key = get_h5_data_keys(dataset_path)\n",
    "print(dataset_path)\n",
    "dataset_full_val = pd.read_hdf(dataset_path, key=key[0])\n",
    "\n",
    "# balance the validation dataset\n",
    "dataset_full_val = normalize_classes(dataset_full_val, label_column=['PHENO'], verbose=True)\n",
    "display(drop_id(dataset_full_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_pdbp has 675 columns, but the combined ppmi dataset has 715. \n",
    "# The columns which are present in ppmi, but not pdbp are below:\n",
    "\n",
    "ppmi_col = set(dataset_full.columns)\n",
    "pdbp_col = set(dataset_full_val.columns)\n",
    "print('ppmi', len(ppmi_col), 'pdbp', len(pdbp_col))\n",
    "display(ppmi_col - pdbp_col)\n",
    "\n",
    "# note, in the `display()` outputs above, the 'ID' property is removed prior to calling display.\n",
    "# the number of columns may differ by 1 if the ID property is cleaned or not. \n",
    "print(\"Before normalization\", dataset_full.shape, dataset_full_val.shape)\n",
    "training_full, validation_full = standardize_for_validation(dataset_full, dataset_full_val)\n",
    "print(\"After normalization\", dataset_full.shape, dataset_full_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c30f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_dataset_columns = list(ppmi_col & pdbp_col) # intersection\n",
    "print(len(used_dataset_columns))\n",
    "\n",
    "dataset_full = dataset_full[used_dataset_columns]\n",
    "dataset_full_val = dataset_full_val[used_dataset_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import time\n",
    "def utils_time_fn(fun, *args, **kwargs):\n",
    "    \"\"\"return (function run time (second), result of function call)\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    result = fun(*args, **kwargs)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    run_time = end_time - start_time\n",
    "    \n",
    "    return (run_time, result)\n",
    "\n",
    "def utils_sk_metrics_to_str(metrics_dict):\n",
    "    \"\"\"convert metrics dict to readable object\"\"\"\n",
    "    rows = []\n",
    "    for key, value in metrics_dict.items():\n",
    "        if key == \"algorithm\":\n",
    "            rows.append(\"{}: {}\".format(key, value))\n",
    "        elif key == \"runtime_s\":\n",
    "            rows.append(\"{}: {:0.3f} seconds\\n\".format(key, value))\n",
    "        else:\n",
    "            rows.append(\"{}: {:0.4f}\".format(key, value))\n",
    "    return str.join(\"\\n\", rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted GenoML\n",
    "import xgboost\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn import svm\n",
    "\n",
    "candidate_algorithms = [\n",
    "    ensemble.AdaBoostRegressor(),\n",
    "    ensemble.BaggingRegressor(),\n",
    "    ensemble.GradientBoostingRegressor(),\n",
    "    ensemble.RandomForestRegressor(n_estimators=10),\n",
    "    linear_model.LinearRegression(),\n",
    "    linear_model.SGDRegressor(),\n",
    "    neighbors.KNeighborsRegressor(),\n",
    "    neural_network.MLPRegressor(),\n",
    "    svm.SVR(gamma='auto'),\n",
    "    xgboost.XGBRegressor(),\n",
    "    xgboost.XGBRFRegressor()\n",
    "]\n",
    "\n",
    "algorithms = {algorithm.__class__.__name__: algorithm for algorithm in candidate_algorithms}\n",
    "print(\"\\n\".join(algorithms.keys()))\n",
    "\n",
    "def evaluate(competing_metrics, algorithm_name, algorithm, x, y):\n",
    "    \"\"\"evaluate how an algorithm does on the provided dataset & generate a pd row\"\"\"\n",
    "    run_time, pred = utils_time_fn(algorithm.predict, x)\n",
    "    metric_results = [metric_func(y, pred) for metric_func in competing_metrics]\n",
    "    \n",
    "    row = [algorithm_name, run_time] + metric_results # + [TN, FN, TP, FP, sensitivity, specificity, PPV, NPV]\n",
    "    return row\n",
    "\n",
    "def process_results(column_names, results):\n",
    "    log_table = pd.DataFrame(data=results, columns=column_names)\n",
    "    best_id = log_table.explained_variance_score.idxmax()\n",
    "    best_algorithm_name = log_table.iloc[best_id].algorithm\n",
    "    best_algorithm = algorithms[best_algorithm_name]\n",
    "    best_algorithm_metrics = log_table.iloc[best_id].to_dict()\n",
    "    \n",
    "    res = {\n",
    "        'log_table': log_table,\n",
    "        'best_id': best_id,\n",
    "        'best_algorithm_name': best_algorithm_name,\n",
    "        'best_algorithm': best_algorithm,\n",
    "        'best_algorithm_metrics': best_algorithm_metrics,\n",
    "    }\n",
    "    \n",
    "    return res\n",
    "\n",
    "def compete(algorithms, x_train, y_train, x_test, y_test, x_addit_test=None, y_addit_test=None):\n",
    "    \"\"\"Compete the algorithms\"\"\"\n",
    "    competing_metrics = [metrics.explained_variance_score, metrics.mean_squared_error,\n",
    "                         metrics.median_absolute_error, metrics.r2_score, metrics.roc_auc_score]\n",
    "    column_names = [\"algorithm\", \"runtime_s\"] + [metric.__name__ for metric in competing_metrics] # + ['TN', 'FN', 'TP', 'FP', 'sensitivity', 'specificity', 'PPV', 'NPV']\n",
    "\n",
    "    results = []\n",
    "    results_val = []\n",
    "    for algorithm_name, algorithm in algorithms.items():\n",
    "\n",
    "        algorithm.fit(x_train, y_train)\n",
    "        \n",
    "        row = evaluate(competing_metrics, algorithm_name, algorithm, x_test, y_test)\n",
    "        results.append(row)\n",
    "        \n",
    "        row = evaluate(competing_metrics, algorithm_name, algorithm, x_addit_test, y_addit_test)\n",
    "        results_val.append(row)\n",
    "    \n",
    "    res = process_results(column_names, results)\n",
    "    results_val = process_results(column_names, results_val)\n",
    "    \n",
    "    return res, results_val\n",
    "\n",
    "def get_split(dataset, splits):\n",
    "    indeces = list(range(0, len(dataset)))\n",
    "    np.random.shuffle(indeces)\n",
    "    subsets = []\n",
    "    for portion in splits:\n",
    "        offset = 0\n",
    "        if (subsets):\n",
    "            offset = len(subsets[-1])\n",
    "\n",
    "        indeces_partition = ceil(len(dataset) * portion)\n",
    "        subset = dataset[offset: min(offset + indeces_partition, len(dataset))]\n",
    "        subset = subset.reset_index(drop=True)\n",
    "        # print(f\"split {portion} - len: {len(subset)} actual: {len(subset) / len(dataset)}\")\n",
    "        # display(subset)\n",
    "        subsets.append(subset)\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd24597",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one validation dataset for all repeats:\n",
    "x_pdbp = dataset_full_val.drop(columns=['ID', 'PHENO'])\n",
    "y_pdbp = dataset_full_val['PHENO']\n",
    "\n",
    "repeats = 5\n",
    "all_logs = []\n",
    "all_logs_val = []\n",
    "for fold in range(repeats):\n",
    "    dataset_test, dataset_train = get_split(dataset_full, [0.15, 0.75])\n",
    "    \n",
    "    x_train = dataset_train.drop(columns=['ID', 'PHENO'])\n",
    "    y_train = dataset_train['PHENO']\n",
    "    x_test = dataset_test.drop(columns=['ID', 'PHENO'])\n",
    "    y_test = dataset_test['PHENO']\n",
    "\n",
    "    result, result_val = compete(algorithms, x_train, y_train, x_test, y_test, x_pdbp, y_pdbp)    \n",
    "    \n",
    "    all_logs.append(result)\n",
    "    all_logs_val.append(result_val)\n",
    "    \n",
    "    print('='*100)\n",
    "    print('\\n', '-'*40, 'ppmi test set', '-'*40)\n",
    "    display(result['log_table'])\n",
    "    print(result['best_algorithm_name'])\n",
    "    display(result['best_algorithm_metrics'])\n",
    "\n",
    "    print('\\n', '-'*40, 'pdbp validation set', '-'*40)\n",
    "    display(result_val['log_table'])\n",
    "    print(result_val['best_algorithm_name'])\n",
    "    display(result_val['best_algorithm_metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7538bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_logs_val[0].keys())\n",
    "print(all_logs_val[0]['best_algorithm_metrics'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "print(all_logs_val[0]['best_algorithm_metrics'].keys())\n",
    "def get_relevant_metrics(metrics: List[str], metrics_dict: Dict):\n",
    "    \n",
    "    results = []\n",
    "    for metric in metrics:\n",
    "        results.append(f\"{metric} {metrics_dict[metric]}\")\n",
    "\n",
    "    return \" \".join(results)\n",
    "\n",
    "print(\"\\n\".join([get_relevant_metrics(['algorithm', 'roc_auc_score'], alg_val_log['best_algorithm_metrics']) for alg_val_log in all_logs_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "most_common_n = Counter([log['best_algorithm_name'] for log in all_logs_val]).most_common(1)\n",
    "most_common, count = most_common_n[0]\n",
    "print(most_common, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f829ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_algorithm = {most_common: algorithms[most_common]}\n",
    "\n",
    "results = []\n",
    "repeats = 5\n",
    "for i in range(repeats):\n",
    "    _, results_val = compete(best_algorithm, x_train, y_train, x_test, y_test, x_pdbp, y_pdbp)\n",
    "    results.append(result_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365da5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean and std auc of the best performing algorithm\n",
    "val_results = [result['best_algorithm_metrics']['roc_auc_score'] for result in results]\n",
    "display(np.mean(val_results))\n",
    "display(np.std(val_results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "998444a8",
   "metadata": {},
   "source": [
    "### Prediction using best algorithm on PPMI Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib as plt\n",
    "alg = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "dataset_train_norm = normalize_classes(dataset_train, 'PHENO', verbose=True)\n",
    "x_train_norm = dataset_train_norm.drop(columns=['ID', 'PHENO'])\n",
    "y_train_norm = dataset_train_norm['PHENO']\n",
    "\n",
    "fitted_alg = alg.fit(x_train, y_train)\n",
    "y_pred = fitted_alg.predict(x_train_norm)\n",
    "\n",
    "svc_disp = RocCurveDisplay.from_predictions(y_train_norm, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d15b1a82",
   "metadata": {},
   "source": [
    "### Prediction using best algorithm on holdout test PPMI Dataset (test set from training stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49139c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = ensemble.GradientBoostingRegressor()\n",
    "fitted_alg = alg.fit(x_train, y_train)\n",
    "# balances predictor classes\n",
    "dataset_test_norm = normalize_classes(dataset_test, 'PHENO', verbose=True)\n",
    "x_test_norm = dataset_test.drop(columns=['ID', 'PHENO'])\n",
    "y_test_norm = dataset_test['PHENO']\n",
    "\n",
    "y_pred = fitted_alg.predict(x_test_norm)\n",
    "\n",
    "# performance on holdout from ppmi dataset\n",
    "svc_disp = RocCurveDisplay.from_predictions(y_test_norm, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa40359c",
   "metadata": {},
   "source": [
    "### Prediction using best algorithm on PDBP Dataset (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc124a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = ensemble.GradientBoostingRegressor()\n",
    "fitted_alg = alg.fit(x_train, y_train)\n",
    "y_pred = fitted_alg.predict(x_pdbp)\n",
    "\n",
    "# performance on normalized \n",
    "svc_disp = RocCurveDisplay.from_predictions(y_pdbp, y_pred)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105bbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nih_fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
