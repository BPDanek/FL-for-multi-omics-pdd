{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l22BMJhu9fPD"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTOSlO9hyXzx"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvCpQKCkycJF",
        "outputId": "64d1e45e-f99c-486d-9cc6-9fa33ad89eae"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from tensorflow_federated.python.simulation.datasets import from_tensor_slices_client_data\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebxuuh074eDr"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from typing import Any, Callable, Dict, List, Tuple, Union\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import torch\n",
        "import collections\n",
        "import h5py\n",
        "\n",
        "# utility functions\n",
        "def build_full_path(base_path, subset=None):\n",
        "    assert subset != None, \"Must provide subset\"\n",
        "    return os.path.join(base_path, subset)\n",
        "\n",
        "def write_json(data: Any, path: str):\n",
        "    \"\"\"Dump json file at path with indent=4\"\"\"\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "def read_json(path: str):\n",
        "    if not os.path.isfile(path):\n",
        "        raise ValueError(f\"{path} does not exist!\")\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def get_h5_data_keys(file_path):\n",
        "    f = h5py.File(file_path, 'r')\n",
        "    keys = list(f.keys())\n",
        "    f.close()\n",
        "    return keys\n",
        "\n",
        "# used for safe displaying of data in ipynb cells\n",
        "# used in exploration and baselines file\n",
        "def drop_id(dataset: pd.DataFrame):\n",
        "    return dataset.drop(columns=['ID'])\n",
        "\n",
        "class GlobalExperimentsConfiguration:\n",
        "\n",
        "    num_rounds = 20\n",
        "\n",
        "    RANDOM_SEED = 1\n",
        "\n",
        "    K = 6\n",
        "\n",
        "    STANDARD = 'standard'\n",
        "    SKLEARN = 'sklearn'\n",
        "\n",
        "    MULTIMODALITY = 'multi_modality'\n",
        "    MULTIMODALITY_LABEL_COL = 'PHENO'\n",
        "    MULTIMODALITY_DATASET_FILES = {\n",
        "        'clinicodemogrpahic_ppmi': 'Clinicodemographic/PPMI_Only_clinical.dataForML.h5',\n",
        "        'transcriptomics_ppmi': 'TRANSCRIPTOMICS_p1E2/PPMI_Only_transcriptomics_only-p1E2.dataForML.h5',\n",
        "        'genetics_ppmi': 'GENETICS_p1E5/PPMI_Only_genetics_p1E5_with_PRS-MAF05.dataForML.h5',\n",
        "\n",
        "        'combined_ppmi': 'Combined_G1E5_O1E2/PPMI-genetic_p1E5_omic_p1E2.dataForML.h5',\n",
        "\n",
        "        'validation_pdbp': 'Validation/validate-PDBP-genetic_p1E5_omic_p1E2.dataForML.h5'\n",
        "    }\n",
        "\n",
        "    MULTIANCESTRY = 'multi_ancestry'\n",
        "    MULTIANCESTRY_DATASET_FOLDER = ''\n",
        "    MULTIANCESTRY_DATASET_FILES = {}\n",
        "\n",
        "    # used for outputting tables\n",
        "    metadata_column_names = ['algorithm_name', 'num_clients', 'split_method', 'val_name']\n",
        "\n",
        "    def __init__(self, base_path: str, experiment_name: str, random_seed=None):\n",
        "        if random_seed:\n",
        "            self.RANDOM_SEED = random_seed\n",
        "        np.random.seed(random_seed)\n",
        "        random.seed(random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "\n",
        "        self.experiment_results = {}\n",
        "        self.experiment_name = experiment_name\n",
        "\n",
        "        # the path which will contain all intermediate experiment files\n",
        "        self.experiment_path = os.path.join(base_path, experiment_name)\n",
        "        os.makedirs(self.experiment_path, exist_ok=True)\n",
        "\n",
        "        self.plots_path = os.path.join(self.experiment_path, 'plots')\n",
        "        os.makedirs(self.plots_path, exist_ok=True)\n",
        "\n",
        "        # avoid duplicating external_val dataset\n",
        "        self.external_val_recorded = False\n",
        "\n",
        "    # @lru_cache(maxsize=10)\n",
        "    def _get_raw_dataset(self, path: str, drop: str):\n",
        "        \"\"\"load the dataset so you can get its size. @lru_cache decorated for speed.\"\"\"\n",
        "        assert '.h5' in path, 'Dataset path does not have a .h5 extension.'\n",
        "        keys = get_h5_data_keys(path)\n",
        "        loaded_dataset = pd.read_hdf(path, key=keys[0])\n",
        "        loaded_dataset = loaded_dataset.drop(columns=['ID'])\n",
        "        return loaded_dataset\n",
        "\n",
        "    def _standardize_for_validation(self, dataset_1: pd.DataFrame, dataset_2: pd.DataFrame):\n",
        "        \"\"\"use the subset of features (columns) which are present in both datasets\"\"\"\n",
        "\n",
        "        shared_dataset_columns = list(set(dataset_1.columns) & set(dataset_2.columns)) # set intersection\n",
        "        print(\"shared columns\", list(shared_dataset_columns))\n",
        "\n",
        "        not_included = (set(dataset_1.columns) | set(dataset_2.columns)) - (set(dataset_1.columns) & set(dataset_2.columns))\n",
        "        print(\"non-shared columns\", len(not_included), list(not_included))\n",
        "\n",
        "        print(f\"shape BEFORE standardization \\n{dataset_1.shape} \\n {dataset_2.shape}\")\n",
        "        dataset_1 = (dataset_1[shared_dataset_columns])\n",
        "        dataset_2 = (dataset_2[shared_dataset_columns])\n",
        "        print(f\"shape AFTER standardization \\n{dataset_1.shape} \\n {dataset_2.shape}\")\n",
        "\n",
        "        return dataset_1, dataset_2\n",
        "\n",
        "    def as_features_labels(self, dataset: pd.DataFrame, label_col: str):\n",
        "        \"\"\"make (feature, label) pairs, where `label_col` represents the label col and all others are features.\n",
        "        Normalize the samples to have balanced value counts if `normalize` is true.\"\"\"\n",
        "\n",
        "        features = dataset.drop(columns=[label_col]).copy().to_numpy()\n",
        "        labels = pd.DataFrame(dataset[label_col].copy()).to_numpy().reshape((-1, ))\n",
        "        return features, labels\n",
        "\n",
        "    def create_experiment(self, dataset_folder: str, dataset: str, split_method: str = STANDARD):\n",
        "        assert split_method == self.STANDARD or split_method == self.SKLEARN, f'Unsupported split_method provided. Recieved {split_method}'\n",
        "\n",
        "        if dataset == self.MULTIMODALITY:\n",
        "\n",
        "            self.LABEL_COL = self.MULTIMODALITY_LABEL_COL\n",
        "\n",
        "            # 0. get the dataset sources\n",
        "            # self.INTERNAL_DATASET = os.path.join(dataset_folder, self.MULTIMODALITY_DATASET_FILES['validation_pdbp'])\n",
        "            self.INTERNAL_DATASET = os.path.join(dataset_folder, self.MULTIMODALITY_DATASET_FILES['combined_ppmi'])\n",
        "\n",
        "            # a secondary dataset only containing validation data for \"extenral validation\" (on a dataset from a different distribution as the internal dataset)\n",
        "            # self.EXTERNAL_DATASET = os.path.join(dataset_folder, self.MULTIMODALITY_DATASET_FILES['combined_ppmi'])\n",
        "            self.EXTERNAL_DATASET = os.path.join(dataset_folder, self.MULTIMODALITY_DATASET_FILES['validation_pdbp'])\n",
        "\n",
        "            # 1. load the dataset from raw file format\n",
        "            full_internal_dataset = self._get_raw_dataset(self.INTERNAL_DATASET, drop='ID')\n",
        "            full_external_dataset = self._get_raw_dataset(self.EXTERNAL_DATASET, drop='ID')\n",
        "            print(\"internal: \", full_internal_dataset.shape)\n",
        "            print(\"external: \", full_external_dataset.shape)\n",
        "\n",
        "            # 2. normalize the feature space the datasets\n",
        "            # Use the subset of features which are shared between the internal and external dataset\n",
        "            # ppmi has 675 columns, but the combined pdbp dataset has 715. Drop the 40 extra columns from pdbp\n",
        "            self.full_internal_dataset, self.full_external_dataset = self._standardize_for_validation(full_internal_dataset, full_external_dataset)\n",
        "            print(\"full internal\", self.full_internal_dataset.info())\n",
        "            print(\"full external\", self.full_external_dataset.info())\n",
        "            # 3. compute k folds for the internal dataset\n",
        "\n",
        "            self._generate_stratified_k_folds(self.full_internal_dataset)\n",
        "            self.full_external_dataset = self.full_external_dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        elif dataset == self.MULTIANCESTRY:\n",
        "            assert False, \"multi ancestry not implemented yet\"\n",
        "\n",
        "        else:\n",
        "            assert False, f\"Unsupported dataset type provided; received {dataset}\"\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _generate_stratified_k_folds(self, df: pd.DataFrame):\n",
        "        \"\"\"Generate k folds of the dataset and store them in the class variable `self.k_fold_indeces`\"\"\"\n",
        "        k_fold_indeces: Dict[int, pd.DataFrame] = dict()\n",
        "\n",
        "        # shuffle the dataframe (should be the only time in the whole experiment we do this.)\n",
        "        df = df.sample(frac=1, replace=False, random_state=self.RANDOM_SEED)\n",
        "\n",
        "        for _, group in df.groupby('PHENO'):\n",
        "\n",
        "            fold_len = len(group) // self.K\n",
        "            start = 0\n",
        "            for fold in range(0, self.K):\n",
        "                end = start + fold_len if fold != self.K - 1 else len(group)\n",
        "\n",
        "                fold_data = group.iloc[start:end]\n",
        "                if fold not in k_fold_indeces:\n",
        "                    k_fold_indeces[fold] = fold_data\n",
        "                else:\n",
        "                    k_fold_indeces[fold] = pd.concat([k_fold_indeces[fold], fold_data])\n",
        "\n",
        "                start = end\n",
        "\n",
        "        # sanity check, since this is such a crucial part of the experimental design\n",
        "        for i, subset_i in k_fold_indeces.items():\n",
        "            for j, subset_j in k_fold_indeces.items():\n",
        "                if i == j: continue\n",
        "                assert set(subset_i.index) & set(subset_j.index) == set(), \"folds have overlapping indeces\"\n",
        "\n",
        "        # all partitions must have approximately similar startification\n",
        "        stratifications_across_folds = [fold_values['PHENO'].value_counts()[0] / fold_values['PHENO'].value_counts()[1] for fold_values in k_fold_indeces.values()]\n",
        "        assert np.std(stratifications_across_folds) < 0.03, f\"folds do not have balanced startification: {stratifications_across_folds}; std: {np.std(stratifications_across_folds)}\"\n",
        "\n",
        "        self.k_fold_indeces = k_fold_indeces\n",
        "\n",
        "    def set_fold(self, fold_idx: int):\n",
        "        \"\"\"Use the provided `fold_idx` as the holdout dataset, use the rest in training\n",
        "\n",
        "        Args:\n",
        "            fold_idx: the fold which is the holdout dataset\n",
        "\n",
        "        Returns:\n",
        "            holdout_dataset, training_dataset\n",
        "        \"\"\"\n",
        "\n",
        "        holdout_idx = fold_idx\n",
        "        holdout_dataset = self.k_fold_indeces[holdout_idx]\n",
        "\n",
        "        trainig_folds = []\n",
        "        for fold_idx, fold in self.k_fold_indeces.items():\n",
        "            if fold_idx != holdout_idx:\n",
        "                trainig_folds.append(fold)\n",
        "\n",
        "        training_dataset = pd.concat(trainig_folds)\n",
        "\n",
        "        self.training_dataset = training_dataset\n",
        "        self.internal_test_dataset = holdout_dataset\n",
        "\n",
        "    def set_validation_dataset(self, ratios = [0.8, 0.2]):\n",
        "        \"\"\"Splits the current training dataset by the ratios, setting `self.training_dataset` to the first split, and `self.validation_dataset` to the second\"\"\"\n",
        "        new_datasets = self.stratified_split(df=self.training_dataset, column='PHENO', ratios=ratios)\n",
        "        assert len(new_datasets) == 2, f\"Validaiton splitting failed; expected 2 new datasets, got {len(new_datasets)}\"\n",
        "\n",
        "        self.training_dataset, self.validation_dataset = new_datasets[0], new_datasets[1]\n",
        "\n",
        "    # def _generate_k_fold_indeces(self, dataset: pd.DataFrame, k: int):\n",
        "    #     \"\"\"generate the indeces for the train/test split and store them in class instance variables\"\"\"\n",
        "    #     kf = KFold(n_splits=k, shuffle=True, random_state=self.RANDOM_SEED)\n",
        "    #     self.train_fold_indices = []\n",
        "    #     self.test_fold_indeces = []\n",
        "    #     self.val_fold_indices = []\n",
        "    #     for train_index, val_index in kf.split(dataset):\n",
        "    #         # we want test set to be 10% of overall dataset\n",
        "    #         # 0.8 * x = 0.1 * 1\n",
        "    #         # x = 0.1/0.8\n",
        "    #         train_test = train_test_split(train_index, test_size=0.125, random_state=self.RANDOM_SEED)\n",
        "    #         train_index, test_index = train_test[0], train_test[1]\n",
        "    #         self.train_fold_indices.append(train_index)\n",
        "    #         self.test_fold_indeces.append(test_index)\n",
        "    #         self.val_fold_indices.append(val_index)\n",
        "\n",
        "    #         print(\"set indeces\", set(train_index) & set(val_index))\n",
        "\n",
        "    # def set_train_dataset(self, fold_idx: int):\n",
        "    #     \"\"\"set the class instance variabel `training_dataset` to the training subset for the provided fold\"\"\"\n",
        "    #     self.training_dataset = self.full_internal_dataset.iloc[self.train_fold_indices[fold_idx]].reset_index(drop=True)\n",
        "    #     self.test_dataset = self.full_internal_dataset.iloc[self.test_fold_indeces[fold_idx]].reset_index(drop=True)\n",
        "\n",
        "    def get_combined_test_dataset(self):\n",
        "        return [\n",
        "            (\"internal test\", self.internal_test_dataset.reset_index(drop=True)),\n",
        "            (\"external test\", self.full_external_dataset.reset_index(drop=True))\n",
        "        ]\n",
        "\n",
        "\n",
        "    # depracated\n",
        "    # def _split_dataframe(self, dataset: pd.DataFrame, ratios: List[int], shuffle: bool, as_intervals: bool) ->  List[Union[pd.DataFrame, Tuple[int]]]:\n",
        "    #     \"\"\"\n",
        "    #     Split the internal dataset by ratios & handle shuffling.\n",
        "    #     Returns either indeces of the dataset splits, or the dataset subsets depending on parameter `as_intervals`.\n",
        "    #     If `as_intervals` is set, one cannot shuffle the dataset, because it would be redundant and is probably a mistake on the programmers part.\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     if shuffle:\n",
        "    #         assert shuffle, \"shuffle depracted\"\n",
        "    #         # assert as_intervals == False, 'There is no need to shuffle the df if we are just returning indeces.'\n",
        "    #         # dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    #     indeces = [0]\n",
        "    #     for i, ratio in enumerate(ratios):\n",
        "\n",
        "    #         last_split = (i == len(ratios) - 1)\n",
        "    #         next_index = indeces[-1] + int(ratio * len(dataset)) if not last_split else len(dataset)\n",
        "    #         indeces.append(next_index)\n",
        "\n",
        "    #     # make sure we don't incorrectly calculate the splits for some reason\n",
        "    #     assert sum([indeces[i+1] - indeces[i] for i in range(0, len(indeces) - 1)]) == len(dataset), f\"Dataset splits do not correctly split the dataset. Expected {len(dataset)} received {sum([indeces[i+1] - indeces[i] for i in range(0, len(indeces) - 1)])}; received ratios {ratios}\"\n",
        "\n",
        "    #     result = []\n",
        "    #     for i in range(0, len(indeces) - 1):\n",
        "    #         start, end = indeces[i], indeces[i + 1]\n",
        "\n",
        "    #         if as_intervals:\n",
        "    #             result.append((start, end))\n",
        "    #         else:\n",
        "    #             result.append(dataset[start: end])\n",
        "\n",
        "    #     return result\n",
        "\n",
        "    def get_stratified_client_subsets(self, dataset: pd.DataFrame, num_clients: int, method: str):\n",
        "        ratios = self.method_to_ratios(method=method, num_clients=num_clients)\n",
        "        return self.stratified_split(df=dataset, column=self.MULTIMODALITY_LABEL_COL, ratios=ratios)\n",
        "\n",
        "    def stratified_split(self, df: pd.DataFrame, column: str, ratios: float) -> List[pd.DataFrame]:\n",
        "        samples = []\n",
        "        groups = df.groupby(column)\n",
        "        for _, group in groups:\n",
        "            shuffled_group = group.sample(frac=1, replace=False, random_state=self.RANDOM_SEED)\n",
        "            n = len(shuffled_group)\n",
        "            offset = 0\n",
        "\n",
        "            indeces = []\n",
        "            for ri, ratio in enumerate(ratios):\n",
        "\n",
        "                start = offset\n",
        "                end = start + int(n * ratio) if ri != len(ratios) - 1 else n\n",
        "                offset = end\n",
        "\n",
        "                indeces.append((start, end))\n",
        "\n",
        "            print(_, indeces)\n",
        "\n",
        "            for i, (start, end) in enumerate(indeces):\n",
        "                stratified_subset = group.iloc[start: end]\n",
        "\n",
        "                if len(samples) == len(ratios):\n",
        "                    samples[i].append(stratified_subset)\n",
        "                else:\n",
        "                    samples.append([stratified_subset])\n",
        "\n",
        "        flattened = []\n",
        "        for sample in samples:\n",
        "            flattened.append(pd.concat([group for group in sample]))\n",
        "\n",
        "        value_proportions = [subset['PHENO'].value_counts()[0] / subset['PHENO'].value_counts()[1] for subset in flattened]\n",
        "        print(value_proportions)\n",
        "        assert np.std(value_proportions) < 0.03, f\"Value counts of stratified dataset inconsistnet. {value_proportions}\"\n",
        "\n",
        "        return flattened\n",
        "\n",
        "    def method_to_ratios(self, method: str, num_clients: int):\n",
        "        assert method in ['uniform', 'linear', 'polynomial', 'exponential'], f'Unsupported method specified for client splits. Recieved {method}'\n",
        "\n",
        "        if method == 'uniform': # 1\n",
        "            ratio_vec = np.ones(num_clients)\n",
        "        elif method == 'linear': # n\n",
        "            ratio_vec = np.linspace(1, num_clients, num=num_clients)\n",
        "        elif method == 'polynomial': # n^2\n",
        "            ratio_vec = np.square(np.linspace(1, num_clients, num=num_clients))\n",
        "        elif method == 'exponential': # e^n\n",
        "            ratio_vec = np.exp(np.linspace(1, num_clients, num=num_clients))\n",
        "\n",
        "        total = sum(ratio_vec)\n",
        "        ratios = ratio_vec / total\n",
        "        return ratios\n",
        "\n",
        "    # def get_client_splits(self, dataset: pd.DataFrame, num_clients: int, method: str, as_intervals=True):\n",
        "    #     \"\"\"returns the indeces of the splits on the dataframe. Does not shuffle the dataframe.\"\"\"\n",
        "    #     ratios = self.method_to_ratios(method=method, num_clients=num_clients)\n",
        "    #     intervals = self._split_dataframe(dataset, ratios=ratios, shuffle=False, as_intervals=as_intervals)\n",
        "    #     return intervals\n",
        "\n",
        "    def nvflare_multi_site_split_json(\n",
        "        self,\n",
        "        data_source_path: List[str],\n",
        "        validation_data_source_path: List[str],\n",
        "        client_splits: List[Tuple[int]],\n",
        "        site_naming_fn: Callable[..., str],\n",
        "        site_config_naming_fn: Callable[..., str],\n",
        "    ) -> List[Tuple[str, dict]]:\n",
        "        \"\"\"build the json for client splits for a single nvflare simulation job provided splits\"\"\"\n",
        "\n",
        "        assert len(data_source_path) == len(client_splits), \"Each client doesnt have its own data path.\"\n",
        "\n",
        "        result_files, result_json = [], []\n",
        "        for index, client_split in enumerate(client_splits):\n",
        "\n",
        "            assert len(client_split) == 2, f'Malformed client split. Received len {len(client_split)}'\n",
        "            start, end = client_split\n",
        "\n",
        "            json_data = {\n",
        "                \"data_path\": data_source_path[index],\n",
        "                \"data_index\": {\n",
        "                    site_naming_fn(index): { # f\"{site_prefix}{site_idx + 1}\"\n",
        "                        \"start\": start,\n",
        "                        \"end\": end\n",
        "                    }\n",
        "                },\n",
        "                \"valid_path\": validation_data_source_path[index]\n",
        "            }\n",
        "\n",
        "            site_file_name = site_config_naming_fn(index)\n",
        "\n",
        "            result_files.append(site_file_name)\n",
        "            result_json.append(json_data)\n",
        "\n",
        "        print(\"resulting files configured\", result_files)\n",
        "        return result_files, result_json\n",
        "\n",
        "    def compute_metrics(self, y_true, y_pred):\n",
        "        return {\n",
        "            'accuracy': metrics.accuracy_score(y_true=y_true, y_pred=y_pred),\n",
        "            'roc_auc_score': metrics.roc_auc_score(y_true=y_true, y_score=y_pred),\n",
        "            'average_precision_score': metrics.average_precision_score(y_true=y_true, y_score=y_pred),\n",
        "            'f0.5': metrics.fbeta_score(y_true=y_true, y_pred=y_pred, beta=0.5),\n",
        "            'f1': metrics.fbeta_score(y_true=y_true, y_pred=y_pred, beta=1),\n",
        "            'f2': metrics.fbeta_score(y_true=y_true, y_pred=y_pred, beta=2),\n",
        "            'log_loss': metrics.log_loss(y_true=y_true, y_pred=y_pred),\n",
        "            'matthews_corrcoef': metrics.matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
        "            # 'num_samples': len(y_true)\n",
        "        }\n",
        "\n",
        "    def add_val_result(self, fold_idx: int, algorithm_name: str, num_clients: str, split_method: str, name: str, y_true, y_pred):\n",
        "\n",
        "        key = (algorithm_name, fold_idx, split_method, num_clients, name)\n",
        "        assert key not in self.experiment_results, f'This result has already been logged. Current results are {list(self.experiment_results.keys())}, received {key}'\n",
        "\n",
        "        self.experiment_results[key] = {\n",
        "            'validation_dataset_name': name,\n",
        "            'metrics': self.compute_metrics(y_true, y_pred),\n",
        "            'size': len(y_true),\n",
        "        }\n",
        "\n",
        "    def k_fold_results_to_stats(self):\n",
        "        k_avgs = collections.defaultdict(list)\n",
        "        for key, val in self.experiment_results.items():\n",
        "            algorithm_name, _, method, num_clients, name = key\n",
        "            newKey = (f\"{algorithm_name}-{method}-{num_clients}-{name}\", name)\n",
        "            k_avgs[newKey].append(val['metrics']['roc_auc_score'])\n",
        "\n",
        "        results = collections.defaultdict(dict)\n",
        "        for key, val in k_avgs.items():\n",
        "            json_id, dataset = key\n",
        "            results[dataset][json_id] = {\n",
        "                'mean': np.mean(val),\n",
        "                'std': np.std(val)\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def add_to_kfold_table(self, algorithm_name: str, num_clients: str, split_method: str, val_name: str, y_true, y_pred):\n",
        "        assert split_method != 'internal_validation' and split_method != 'internal_validation', f'incorrect val name, received {val_name}'\n",
        "        data = self.compute_metrics(y_true, y_pred)\n",
        "        row_data = [algorithm_name, num_clients, split_method, val_name]\n",
        "        row_data.extend(data.values())\n",
        "\n",
        "        # record validation only once\n",
        "        if val_name == 'external test' and self.external_val_recorded:\n",
        "            return\n",
        "\n",
        "        if not hasattr(self, 'kfold_table'):\n",
        "            col_names = self.metadata_column_names.copy()\n",
        "\n",
        "            all_cols = col_names.copy() + list(data.keys())\n",
        "            self.kfold_table = pd.DataFrame([row_data], columns=all_cols)\n",
        "        else:\n",
        "            self.kfold_table.loc[len(self.kfold_table.index)] = row_data\n",
        "\n",
        "    def write_results(self, path: str):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        write_path = os.path.join(path, f\"{self.experiment_name}.csv\")\n",
        "        self.kfold_table.to_csv(write_path, index=False)\n",
        "        return write_path\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkUGymlFlTEr"
      },
      "source": [
        "## Define experiment folder in collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq8UldiuW66D",
        "outputId": "a6a22ec8-b8f2-4e67-ac81-8b2f031b31bb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89ld71rTllhr",
        "outputId": "22ee61f7-22a0-4ccd-9b0e-5c7eb5b84ca7"
      },
      "outputs": [],
      "source": [
        "current_experiment = GlobalExperimentsConfiguration(\n",
        "    base_path='/content/drive/MyDrive/collab_nih_fl',\n",
        "    experiment_name='federated_MLP_regression_tf',\n",
        "    random_seed=0\n",
        ")\n",
        "\n",
        "current_experiment.create_experiment(\n",
        "    dataset_folder='/content/drive/MyDrive/collab_nih_fl/data',\n",
        "    dataset=GlobalExperimentsConfiguration.MULTIMODALITY,\n",
        "    split_method=GlobalExperimentsConfiguration.SKLEARN\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewJ3hkQlniFl"
      },
      "source": [
        "## Load and store the experiment datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqsdAY3rkv2w"
      },
      "source": [
        "## Strategy to make dataset split configs (must have parity to local version of experiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sivhgDxF9TaD"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from tensorflow_federated.python.simulation.datasets import from_tensor_slices_client_data\n",
        "from tensorflow_federated.python.learning.models import variable\n",
        "import pickle\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKlw3gRmpQc7"
      },
      "outputs": [],
      "source": [
        "# # tf training data\n",
        "# NUM_EPOCHS = 10\n",
        "# BATCH_SIZE = 60\n",
        "# SHUFFLE_BUFFER = 100\n",
        "# PREFETCH_BUFFER = 10\n",
        "\n",
        "# # FL communication rounds:\n",
        "# NUM_ROUNDS = 10\n",
        "\n",
        "# # normalize feature ranges since this is will affect neural networks\n",
        "# def min_max_normalized(data):\n",
        "#     return data\n",
        "#     # normalized already\n",
        "#     # col_max = np.max(data, axis=0)\n",
        "#     # col_min = np.min(data, axis=0)\n",
        "#     # return np.divide(data - col_min, col_max - col_min)\n",
        "\n",
        "# for fold_idx in range(current_experiment.K):\n",
        "#     current_experiment.set_train_dataset(fold_idx=fold_idx)\n",
        "#     current_experiment.set_combined_validation_dataset(fold_idx=fold_idx)\n",
        "\n",
        "#     current_experiment.training_dataset = min_max_normalized(current_experiment.training_dataset)\n",
        "#     current_experiment.combined_validation_dataset = [(n, min_max_normalized(d)) for n, d in current_experiment.combined_validation_dataset]\n",
        "\n",
        "#     # generate data for several site configurations\n",
        "#     # each configuration is a json, so there is no duplication of underlying data\n",
        "#     site_configs = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "#     site_prefixes = [\"site-\"] * len(site_configs)\n",
        "#     split_methods = [\"uniform\"] * len(site_configs)\n",
        "\n",
        "#     experiment_results = []\n",
        "\n",
        "#     for i in range(len(site_configs)):\n",
        "#         num_clients, site_prefix, split_method = site_configs[i], site_prefixes[i], split_methods[i]\n",
        "\n",
        "#         # training splits\n",
        "#         client_splits = current_experiment.get_client_splits(\n",
        "#             dataset=current_experiment.training_dataset,\n",
        "#             num_clients=num_clients,\n",
        "#             method=split_method,\n",
        "#             as_intervals=False\n",
        "#         )\n",
        "\n",
        "#         def site_naming_fn(site_index):\n",
        "#             \"\"\"Used for naming files in the client data split json\"\"\"\n",
        "#             return f\"{site_prefix}{site_index + 1}\"\n",
        "\n",
        "#         # columns per sample includes label\n",
        "#         samples_per_client, columns_per_sample = client_splits[0].shape\n",
        "#         client_dict = {}\n",
        "#         for site_idx, client_split in enumerate(client_splits):\n",
        "\n",
        "#           # take the PHENO column and make it the last column in\n",
        "#           # preparation for converting this to a python list\n",
        "#           features = client_split.drop(columns=['PHENO']).to_numpy()\n",
        "#           labels = pd.DataFrame(client_split['PHENO']).to_numpy()\n",
        "\n",
        "#           client_dict[site_naming_fn(site_idx)] = collections.OrderedDict(\n",
        "#               x = features.tolist(),\n",
        "#               y = labels.tolist(),\n",
        "#           )\n",
        "\n",
        "#         client_splits = from_tensor_slices_client_data.TestClientData(client_dict)\n",
        "\n",
        "#         def preprocess(dataset):\n",
        "\n",
        "#           def batch_format_fn(element):\n",
        "#             \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "#             new = collections.OrderedDict(\n",
        "#                 x=tf.cast(tf.reshape(element['x'], [-1, columns_per_sample - 1]), tf.float32),\n",
        "#                 y=tf.cast(tf.reshape(element['y'], [-1, 1]), tf.float32))\n",
        "\n",
        "#             return new\n",
        "\n",
        "#           return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=current_experiment.RANDOM_SEED).batch(\n",
        "#               BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "#         # used to compute elment structure after processing\n",
        "#         # Used to compute the element spec in the TestClientData class, just replicated\n",
        "#         # for batch_format_fn data\n",
        "#         example_dataset = client_splits.create_tf_dataset_for_client(client_splits.client_ids[0])\n",
        "#         preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "#         def make_federated_data(client_data, client_ids):\n",
        "#           return [\n",
        "#               preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "#               for x in client_ids\n",
        "#           ]\n",
        "\n",
        "#         # sample_clients = client_splits.client_ids\n",
        "#         federated_train_data = make_federated_data(client_splits, client_splits.client_ids)\n",
        "\n",
        "#         # multi-layer perceptron regression\n",
        "#         def create_keras_model():\n",
        "#           return tf.keras.models.Sequential([\n",
        "#               tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(columns_per_sample - 1,)),\n",
        "#               tf.keras.layers.Dense(100, activation='relu', input_dim=columns_per_sample - 1,),\n",
        "#               tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "#           ])\n",
        "\n",
        "#         loss_fn = lambda: tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "#         metrics_list = lambda: [tf.keras.metrics.BinaryCrossentropy(), tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n",
        "\n",
        "#         def model_fn(): #-> variable.VariableModel:\n",
        "#           # We _must_ create a new model here, and _not_ capture it from an external\n",
        "#           # scope. TFF will call this within different graph contexts.\n",
        "#           keras_model = create_keras_model()\n",
        "\n",
        "#           return tff.learning.models.from_keras_model(\n",
        "#               keras_model,\n",
        "#               input_spec=preprocessed_example_dataset.element_spec,\n",
        "#               loss=loss_fn(),\n",
        "#               metrics=metrics_list()\n",
        "#             )\n",
        "\n",
        "#         training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "#           model_fn,\n",
        "#           client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02), # 0.02\n",
        "#           server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "#         train_state = training_process.initialize()\n",
        "#         best_model_weights = None\n",
        "#         best_loss = float('inf')\n",
        "#         for round_num in range(0, NUM_ROUNDS):\n",
        "#           result = training_process.next(train_state, federated_train_data)\n",
        "#           train_state = result.state\n",
        "#           train_metrics = result.metrics\n",
        "#           print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "\n",
        "#           # get test data\n",
        "#           test_dataset = current_experiment.test_dataset\n",
        "#           X, y = current_experiment.as_features_labels(test_dataset, current_experiment.LABEL_COL)\n",
        "\n",
        "#           # copy over model weights from trainer\n",
        "#           model = create_keras_model()\n",
        "#           model.set_weights(training_process.get_model_weights(train_state)[0])\n",
        "\n",
        "#           y_pred = model.predict(X)\n",
        "#           # y_pred = y_pred.astype(int)\n",
        "\n",
        "#           print(y.dtype)\n",
        "#           print(y_pred.dtype)\n",
        "\n",
        "#           bce = loss_fn()\n",
        "#           loss = bce(y, y_pred)\n",
        "#           auc =  metrics.roc_auc_score(y, y_pred)\n",
        "#           print('test round {:2d}, loss={:.3f}, auc={:.3f}'.format(round_num, loss, auc))\n",
        "\n",
        "#           if loss < best_loss:\n",
        "#             best_loss = loss\n",
        "#             best_model_weights = training_process.get_model_weights(train_state)\n",
        "#             pickle.dump(best_model_weights, open(\"current_best_model_weights.pkl\", \"wb\"))\n",
        "#             pickle.dump(current_experiment, open(\"current_experiment.pkl\", \"wb\"))\n",
        "\n",
        "#             checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#               filepath=checkpoint_prefix,\n",
        "#               save_weights_only=True)\n",
        "\n",
        "\n",
        "#         # evaluation_process = tff.learning.algorithms.build_fed_eval(model_fn)\n",
        "#         # evaluation_state = evaluation_process.initialize()\n",
        "\n",
        "#         # take trained weights & use them for evaluation\n",
        "#         model_weights = best_model_weights\n",
        "#         # Save the entire model, including architecture and weights\n",
        "#         model.save(\"model.h5\")\n",
        "\n",
        "#         # Load the model with its architecture and weights\n",
        "#         model = tf.keras.models.load_model(\"model.h5\")\n",
        "\n",
        "#         # get the evaluation datasets\n",
        "#         validation_results = []\n",
        "#         val_dataset = current_experiment.combined_validation_dataset\n",
        "#         val_dataset.append(('test', current_experiment.test_dataset))\n",
        "#         for data in val_dataset:\n",
        "#             name, dataset = data\n",
        "\n",
        "#             X, y = current_experiment.as_features_labels(dataset, current_experiment.LABEL_COL)\n",
        "\n",
        "#             y_pred = model.predict(X)\n",
        "#             y_pred = y_pred.reshape(-1)\n",
        "#             y_pred = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "#             # client_splits = current_experiment.get_client_splits(\n",
        "#             #     dataset=dataset,\n",
        "#             #     num_clients=1,\n",
        "#             #     method=split_method,\n",
        "#             #     as_intervals=False\n",
        "#             # )\n",
        "\n",
        "#             # client_dict = {}\n",
        "#             # for site_idx, client_split in enumerate(client_splits):\n",
        "\n",
        "#             #   # take the PHENO column and make it the last column in\n",
        "#             #   # preparation for converting this to a python list\n",
        "#             #   features = client_split.drop(columns=['PHENO']).to_numpy()\n",
        "#             #   labels = pd.DataFrame(client_split['PHENO']).to_numpy()\n",
        "\n",
        "#             #   client_dict[site_naming_fn(site_idx)] = collections.OrderedDict(\n",
        "#             #       x = features.tolist(),\n",
        "#             #       y = labels.tolist(),\n",
        "#             #   )\n",
        "\n",
        "#             # client_splits = from_tensor_slices_client_data.TestClientData(client_dict)\n",
        "\n",
        "#             # federated_val_data = make_federated_data(client_splits, client_splits.client_ids)\n",
        "#             # # sample_clients = client_splits.client_ids\n",
        "\n",
        "#             # state, metrics = evaluation_process.next(evaluation_state, federated_val_data)\n",
        "\n",
        "#             # current_experiment.add_val_result(\n",
        "#             #     fold_idx=fold_idx,\n",
        "#             #     num_clients=num_clients,\n",
        "#             #     split_method=split_method,\n",
        "#             #     name=name,\n",
        "#             #     auc=float(metrics['client_work']['eval']['total_rounds_metrics']['auc']),\n",
        "#             #     size=float(metrics['client_work']['eval']['total_rounds_metrics']['num_examples'])\n",
        "#             # )\n",
        "#             current_experiment.add_to_kfold_table(\n",
        "#                 algorithm_name='Federated MLP',\n",
        "#                 num_clients=num_clients,\n",
        "#                 split_method=split_method,\n",
        "#                 val_name=name,\n",
        "#                 y_true=y,\n",
        "#                 y_pred=y_pred\n",
        "#             )\n",
        "#     break\n",
        "# print(current_experiment.experiment_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzak5UscI1iz",
        "outputId": "f0d7abd6-1db5-44c0-80c3-bdb1e207623f"
      },
      "outputs": [],
      "source": [
        "# tf training data\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 500\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "# FL communication rounds:\n",
        "NUM_ROUNDS = current_experiment.num_rounds\n",
        "\n",
        "# normalize feature ranges since this is will affect neural networks\n",
        "def min_max_normalized(data):\n",
        "    return data\n",
        "    # normalized already\n",
        "    # col_max = np.max(data, axis=0)\n",
        "    # col_min = np.min(data, axis=0)\n",
        "    # return np.divide(data - col_min, col_max - col_min)\n",
        "\n",
        "for fold_idx in range(current_experiment.K):\n",
        "    current_experiment.set_fold(fold_idx=fold_idx)\n",
        "    current_experiment.set_validation_dataset()\n",
        "\n",
        "    # generate data for several site configurations\n",
        "    # each configuration is a json, so there is no duplication of underlying data\n",
        "    site_configs = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "    site_prefixes = [\"site-\"] * len(site_configs)\n",
        "    split_methods = [\"uniform\"] * len(site_configs)\n",
        "\n",
        "    experiment_results = []\n",
        "\n",
        "    for i in range(len(site_configs)):\n",
        "        num_clients, site_prefix, split_method = site_configs[i], site_prefixes[i], split_methods[i]\n",
        "\n",
        "        # training splits\n",
        "        client_folds = current_experiment.get_stratified_client_subsets(\n",
        "            current_experiment.training_dataset,\n",
        "            num_clients=num_clients,\n",
        "            method=split_method\n",
        "        )\n",
        "\n",
        "        def site_naming_fn(site_index):\n",
        "            \"\"\"Used for naming files in the client data split json\"\"\"\n",
        "            return f\"{site_prefix}{site_index + 1}\"\n",
        "\n",
        "        # columns per sample includes label\n",
        "        _, columns_per_sample = client_folds[0].shape\n",
        "        client_dict = {}\n",
        "        for site_idx, fold in enumerate(client_folds):\n",
        "\n",
        "          # take the PHENO column and make it the last column in\n",
        "          # preparation for converting this to a python list\n",
        "          features = fold.drop(columns=['PHENO']).to_numpy()\n",
        "          labels = pd.DataFrame(fold['PHENO']).to_numpy()\n",
        "\n",
        "          client_dict[site_naming_fn(site_idx)] = collections.OrderedDict(\n",
        "              x = features.tolist(),\n",
        "              y = labels.tolist(),\n",
        "          )\n",
        "\n",
        "        client_splits = from_tensor_slices_client_data.TestClientData(client_dict)\n",
        "\n",
        "        def preprocess(dataset):\n",
        "\n",
        "          def batch_format_fn(element):\n",
        "            \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "            new = collections.OrderedDict(\n",
        "                x=tf.cast(tf.reshape(element['x'], [-1, columns_per_sample - 1]), tf.float32),\n",
        "                y=tf.cast(tf.reshape(element['y'], [-1, 1]), tf.float32))\n",
        "\n",
        "            return new\n",
        "\n",
        "          return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=current_experiment.RANDOM_SEED).batch(\n",
        "              BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "        # used to compute elment structure after processing\n",
        "        # Used to compute the element spec in the TestClientData class, just replicated\n",
        "        # for batch_format_fn data\n",
        "        example_dataset = client_splits.create_tf_dataset_for_client(client_splits.client_ids[0])\n",
        "        preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "        def make_federated_data(client_data, client_ids):\n",
        "          return [\n",
        "              preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "              for x in client_ids\n",
        "          ]\n",
        "\n",
        "        # sample_clients = client_splits.client_ids\n",
        "        federated_train_data = make_federated_data(client_splits, client_splits.client_ids)\n",
        "\n",
        "        # MLP regression\n",
        "        def create_keras_model():\n",
        "          return tf.keras.models.Sequential([\n",
        "              tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(columns_per_sample - 1,)),\n",
        "              tf.keras.layers.Dense(100, activation='relu', input_dim=columns_per_sample - 1,),\n",
        "              tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "          ])\n",
        "\n",
        "        loss_fn = lambda: tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "        metrics_list = lambda: [tf.keras.metrics.BinaryCrossentropy(), tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n",
        "\n",
        "        def model_fn(): #-> variable.VariableModel:\n",
        "          # We _must_ create a new model here, and _not_ capture it from an external\n",
        "          # scope. TFF will call this within different graph contexts.\n",
        "          keras_model = create_keras_model()\n",
        "\n",
        "          return tff.learning.models.from_keras_model(\n",
        "              keras_model,\n",
        "              input_spec=preprocessed_example_dataset.element_spec,\n",
        "              loss=loss_fn(),\n",
        "              metrics=metrics_list()\n",
        "            )\n",
        "\n",
        "        training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "          model_fn,\n",
        "          client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02), # 0.02\n",
        "          server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "        train_state = training_process.initialize()\n",
        "        best_model_weights = None\n",
        "        best_loss = float('inf')\n",
        "        for round_num in range(0, NUM_ROUNDS):\n",
        "          result = training_process.next(train_state, federated_train_data)\n",
        "          train_state = result.state\n",
        "          train_metrics = result.metrics\n",
        "          print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "\n",
        "          # get test data\n",
        "          validation_dataset = current_experiment.validation_dataset\n",
        "          X, y = current_experiment.as_features_labels(validation_dataset, current_experiment.LABEL_COL)\n",
        "\n",
        "          # copy over model weights from trainer\n",
        "          model = create_keras_model()\n",
        "          model.set_weights(training_process.get_model_weights(train_state)[0])\n",
        "\n",
        "          y_pred = model.predict(X)\n",
        "          # y_pred = y_pred.astype(int)\n",
        "\n",
        "          print(y.dtype)\n",
        "          print(y_pred.dtype)\n",
        "\n",
        "          bce = loss_fn()\n",
        "          loss = bce(y, y_pred)\n",
        "          auc =  metrics.roc_auc_score(y, y_pred)\n",
        "          print('test round {:2d}, loss={:.3f}, auc={:.3f}'.format(round_num, loss, auc))\n",
        "\n",
        "          if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model_weights = training_process.get_model_weights(train_state)\n",
        "            pickle.dump(best_model_weights, open(\"current_best_model_weights.pkl\", \"wb\"))\n",
        "            pickle.dump(current_experiment, open(\"current_experiment.pkl\", \"wb\"))\n",
        "\n",
        "\n",
        "        # evaluation_process = tff.learning.algorithms.build_fed_eval(model_fn)\n",
        "        # evaluation_state = evaluation_process.initialize()\n",
        "\n",
        "        # take trained weights & use them for evaluation\n",
        "        model_weights = best_model_weights\n",
        "        import pickle\n",
        "        pickle.dump(model_weights, open(\"model_weights.pkl\", \"wb\"))\n",
        "        print(len(model_weights))\n",
        "        # evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
        "        model = create_keras_model()\n",
        "        model.set_weights(model_weights[0])\n",
        "\n",
        "\n",
        "        # get the evaluation datasets\n",
        "        validation_results = []\n",
        "        for name, dataset in current_experiment.get_combined_test_dataset():\n",
        "\n",
        "            X, y = current_experiment.as_features_labels(dataset, current_experiment.LABEL_COL)\n",
        "\n",
        "            y_pred = model.predict(X)\n",
        "            y_pred = y_pred.reshape(-1)\n",
        "            y_pred = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "            current_experiment.add_to_kfold_table(\n",
        "                algorithm_name='Federated MLP Regression',\n",
        "                num_clients=num_clients,\n",
        "                split_method=split_method,\n",
        "                val_name=name,\n",
        "                y_true=y,\n",
        "                y_pred=y_pred\n",
        "            )\n",
        "\n",
        "print(current_experiment.experiment_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "stm0fspZbnLY",
        "outputId": "4228c7ce-3cf1-4c82-a249-e2e8623ee1a5"
      },
      "outputs": [],
      "source": [
        "internal_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'internal test'].groupby(current_experiment.metadata_column_names)\n",
        "display(internal_only.mean())\n",
        "exteral_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'external test'].groupby(current_experiment.metadata_column_names)\n",
        "display(exteral_only.mean())\n",
        "exteral_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'test'].groupby(current_experiment.metadata_column_names)\n",
        "display(exteral_only.mean())\n",
        "current_experiment.write_results('/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jny-FN0rS98U"
      },
      "outputs": [],
      "source": [
        "# tf training data\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 60\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "NUM_ROUNDS = 150\n",
        "\n",
        "\n",
        "current_experiment.set_train_dataset(fold_idx=1)\n",
        "current_experiment.set_combined_validation_dataset(fold_idx=1)\n",
        "\n",
        "current_experiment.training_dataset = current_experiment.training_dataset\n",
        "current_experiment.combined_validation_dataset = [(n, d) for n, d in current_experiment.combined_validation_dataset]\n",
        "\n",
        "num_clients, site_prefix, split_method = 4, \"site-\", \"uniform\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj-GyQqW6MEC"
      },
      "outputs": [],
      "source": [
        "# tf training data\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 60\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "NUM_ROUNDS = 150\n",
        "\n",
        "\n",
        "current_experiment.set_train_dataset(fold_idx=fold_idx)\n",
        "current_experiment.set_combined_validation_dataset(fold_idx=fold_idx)\n",
        "\n",
        "current_experiment.training_dataset = min_max_normalized(current_experiment.training_dataset)\n",
        "current_experiment.combined_validation_dataset = [(n, min_max_normalized(d)) for n, d in current_experiment.combined_validation_dataset]\n",
        "\n",
        "# generate data for several site configurations\n",
        "# each configuration is a json, so there is no duplication of underlying data\n",
        "site_configs = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "site_prefixes = [\"site-\"] * len(site_configs)\n",
        "split_methods = [\"uniform\"] * len(site_configs)\n",
        "\n",
        "experiment_results = []\n",
        "\n",
        "i = 1\n",
        "num_clients, site_prefix, split_method = site_configs[i], site_prefixes[i], split_methods[i]\n",
        "\n",
        "# training splits\n",
        "client_splits = current_experiment.get_client_splits(\n",
        "    dataset=current_experiment.training_dataset,\n",
        "    num_clients=num_clients,\n",
        "    method=split_method,\n",
        "    as_intervals=False\n",
        ")\n",
        "\n",
        "def site_naming_fn(site_index):\n",
        "    \"\"\"Used for naming files in the client data split json\"\"\"\n",
        "    return f\"{site_prefix}{site_index + 1}\"\n",
        "\n",
        "# columns per sample includes label\n",
        "samples_per_client, columns_per_sample = client_splits[0].shape\n",
        "client_dict = {}\n",
        "for site_idx, client_split in enumerate(client_splits):\n",
        "\n",
        "  # take the PHENO column and make it the last column in\n",
        "  # preparation for converting this to a python list\n",
        "  features = client_split.drop(columns=['PHENO']).to_numpy()\n",
        "  labels = pd.DataFrame(client_split['PHENO']).to_numpy()\n",
        "\n",
        "  client_dict[site_naming_fn(site_idx)] = collections.OrderedDict(\n",
        "      x = features.tolist(),\n",
        "      y = labels.tolist(),\n",
        "  )\n",
        "\n",
        "client_splits = from_tensor_slices_client_data.TestClientData(client_dict)\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    new = collections.OrderedDict(\n",
        "        x=tf.cast(tf.reshape(element['x'], [-1, columns_per_sample - 1]), tf.float32),\n",
        "        y=tf.cast(tf.reshape(element['y'], [-1, 1]), tf.float32))\n",
        "\n",
        "    return new\n",
        "\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=current_experiment.RANDOM_SEED).batch(\n",
        "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "# used to compute elment structure after processing\n",
        "# Used to compute the element spec in the TestClientData class, just replicated\n",
        "# for batch_format_fn data\n",
        "example_dataset = client_splits.create_tf_dataset_for_client(client_splits.client_ids[0])\n",
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "  ]\n",
        "\n",
        "# sample_clients = client_splits.client_ids\n",
        "federated_train_data = make_federated_data(client_splits, client_splits.client_ids)\n",
        "\n",
        "# multi-layer perceptron regression\n",
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(columns_per_sample - 1,)),\n",
        "      tf.keras.layers.Dense(100, activation='relu', input_dim=columns_per_sample - 1,),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "loss_fn = lambda: tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "metrics_list = lambda: [tf.keras.metrics.BinaryCrossentropy(), tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n",
        "\n",
        "def model_fn(): #-> variable.VariableModel:\n",
        "  # We _must_ create a new model here, and _not_ capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "  keras_model = create_keras_model()\n",
        "\n",
        "  return tff.learning.models.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=loss_fn(),\n",
        "      metrics=metrics_list()\n",
        "    )\n",
        "\n",
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "  model_fn,\n",
        "  client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02), # 0.02\n",
        "  server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "train_state = training_process.initialize()\n",
        "best_model_weights = None\n",
        "best_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDfiXhTfIBpx"
      },
      "outputs": [],
      "source": [
        "training_process.__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8I1iML9ITyh"
      },
      "outputs": [],
      "source": [
        "training_process.get_hparams()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOIu5CKBgNfl"
      },
      "outputs": [],
      "source": [
        "# model_weights = best_model_weights\n",
        "# print(len(model_weights))\n",
        "# # evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
        "# model = create_keras_model()\n",
        "# model.set_weights(model_weights.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1lEnPzP6ywa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# evaluation_process = tff.learning.algorithms.build_fed_eval(model_fn)\n",
        "# evaluation_state = evaluation_process.initialize()\n",
        "\n",
        "# take trained weights & use them for evaluation\n",
        "import pickle\n",
        "model_weights = pickle.load(open(\"model_weights.pkl\", \"rb\"))\n",
        "print(len(model_weights))\n",
        "# evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
        "model = create_keras_model()\n",
        "# model.set_weights(model_weights[0])\n",
        "model.set_weights(training_process.get_model_weights(train_state)[0])\n",
        "\n",
        "\n",
        "# get the evaluation datasets\n",
        "validation_results = []\n",
        "name, dataset = current_experiment.combined_validation_dataset[1]\n",
        "\n",
        "X, y = current_experiment.as_features_labels(dataset, current_experiment.LABEL_COL)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "y_pred = y_pred.reshape(-1)\n",
        "for i in range(0, len(y)):\n",
        "  print(y[i], y_pred[i])\n",
        "y_pred = (y_pred > 0.5).astype(\"int32\")\n",
        "for i in range(0, len(y)):\n",
        "  print(y[i], y_pred[i])\n",
        "    # client_splits = current_experiment.get_client_splits(\n",
        "    #     dataset=dataset,\n",
        "    #     num_clients=1,\n",
        "    #     method=split_method,\n",
        "    #     as_intervals=False\n",
        "    # )\n",
        "\n",
        "    # client_dict = {}\n",
        "    # for site_idx, client_split in enumerate(client_splits):\n",
        "\n",
        "    #   # take the PHENO column and make it the last column in\n",
        "    #   # preparation for converting this to a python list\n",
        "    #   features = client_split.drop(columns=['PHENO']).to_numpy()\n",
        "    #   labels = pd.DataFrame(client_split['PHENO']).to_numpy()\n",
        "\n",
        "    #   client_dict[site_naming_fn(site_idx)] = collections.OrderedDict(\n",
        "    #       x = features.tolist(),\n",
        "    #       y = labels.tolist(),\n",
        "    #   )\n",
        "\n",
        "    # client_splits = from_tensor_slices_client_data.TestClientData(client_dict)\n",
        "\n",
        "    # federated_val_data = make_federated_data(client_splits, client_splits.client_ids)\n",
        "    # # sample_clients = client_splits.client_ids\n",
        "\n",
        "    # state, metrics = evaluation_process.next(evaluation_state, federated_val_data)\n",
        "\n",
        "    # current_experiment.add_val_result(\n",
        "    #     fold_idx=fold_idx,\n",
        "    #     num_clients=num_clients,\n",
        "    #     split_method=split_method,\n",
        "    #     name=name,\n",
        "    #     auc=float(metrics['client_work']['eval']['total_rounds_metrics']['auc']),\n",
        "    #     size=float(metrics['client_work']['eval']['total_rounds_metrics']['num_examples'])\n",
        "    # )\n",
        "    # current_experiment.add_to_kfold_table(\n",
        "    #     algorithm_name='Federated Logistic Regression',\n",
        "    #     num_clients=num_clients,\n",
        "    #     split_method=split_method,\n",
        "    #     val_name=name,\n",
        "    #     y_true=y,\n",
        "    #     y_pred=y_pred\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg5FINNMhaCc"
      },
      "outputs": [],
      "source": [
        "# current_experiment.kfold_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQEeXi2uTAU8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
