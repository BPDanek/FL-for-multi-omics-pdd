{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476c15bc",
   "metadata": {},
   "source": [
    "File computing the baseline performance of centralized algorithms from: https://www.nature.com/articles/s41531-022-00288-w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15372da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_modality_fl.utils.data_management import GlobalExperimentsConfiguration, write_json, read_json\n",
    "\n",
    "current_experiment = GlobalExperimentsConfiguration(\n",
    "    base_path=os.path.join(os.getcwd(), os.path.join('multi_modality_fl', 'experiments')),\n",
    "    experiment_name='baselines',\n",
    "    random_seed=0\n",
    ")\n",
    "\n",
    "current_experiment.initialize_data_splits(\n",
    "    dataset_folder='/Users/benjamindanek/Code/federated_learning_multi_modality_ancestry/data',\n",
    "    dataset=GlobalExperimentsConfiguration.MULTIMODALITY,\n",
    "    split_method=GlobalExperimentsConfiguration.SKLEARN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import time\n",
    "def utils_time_fn(fun, *args, **kwargs):\n",
    "    \"\"\"return (function run time (second), result of function call)\"\"\"\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    result = fun(*args, **kwargs)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    run_time = end_time - start_time\n",
    "    \n",
    "    return (run_time, result)\n",
    "\n",
    "def utils_sk_metrics_to_str(metrics_dict):\n",
    "    \"\"\"convert metrics dict to readable object\"\"\"\n",
    "    rows = []\n",
    "    for key, value in metrics_dict.items():\n",
    "        if key == \"algorithm\":\n",
    "            rows.append(\"{}: {}\".format(key, value))\n",
    "        elif key == \"runtime_s\":\n",
    "            rows.append(\"{}: {:0.3f} seconds\\n\".format(key, value))\n",
    "        else:\n",
    "            rows.append(\"{}: {:0.4f}\".format(key, value))\n",
    "    return str.join(\"\\n\", rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted GenoML\n",
    "import xgboost\n",
    "from sklearn import discriminant_analysis, ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn import svm\n",
    "\n",
    "candidate_algorithms = [\n",
    "    linear_model.LogisticRegression(solver='lbfgs'),\n",
    "    ensemble.RandomForestClassifier(n_estimators=100),\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    linear_model.SGDClassifier(loss='modified_huber'),\n",
    "    svm.SVC(probability=True, gamma='scale'),\n",
    "    neural_network.MLPClassifier(),\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    xgboost.XGBClassifier(),\n",
    "    xgboost.XGBRFClassifier()\n",
    "]\n",
    "\n",
    "algorithms = {algorithm.__class__.__name__: algorithm for algorithm in candidate_algorithms}\n",
    "print(\"\\n\".join(algorithms.keys()))\n",
    "\n",
    "def evaluate(competing_metrics, algorithm_name, algorithm, x, y):\n",
    "    \"\"\"evaluate how an algorithm does on the provided dataset & generate a pd row\"\"\"\n",
    "    run_time, pred = utils_time_fn(algorithm.predict, x)\n",
    "    metric_results = [metric_func(y, pred) for metric_func in competing_metrics]\n",
    "    \n",
    "    row = [algorithm_name, run_time] + metric_results # + [TN, FN, TP, FP, sensitivity, specificity, PPV, NPV]\n",
    "    return row, pred\n",
    "\n",
    "def process_results(column_names, results):\n",
    "    log_table = pd.DataFrame(data=results, columns=column_names)\n",
    "    best_id = log_table.explained_variance_score.idxmax()\n",
    "    best_algorithm_name = log_table.iloc[best_id].algorithm\n",
    "    best_algorithm = algorithms[best_algorithm_name]\n",
    "    best_algorithm_metrics = log_table.iloc[best_id].to_dict()\n",
    "    \n",
    "    res = {\n",
    "        'log_table': log_table,\n",
    "        'best_id': best_id,\n",
    "        'best_algorithm_name': best_algorithm_name,\n",
    "        'best_algorithm': best_algorithm,\n",
    "        'best_algorithm_metrics': best_algorithm_metrics,\n",
    "    }\n",
    "    \n",
    "    return res\n",
    "\n",
    "def compete(fold_idx, algorithms, x_train, y_train, x_test, y_test, x_addit_test=None, y_addit_test=None):\n",
    "    \"\"\"Compete the algorithms\"\"\"\n",
    "    competing_metrics = [metrics.explained_variance_score, metrics.mean_squared_error,\n",
    "                         metrics.median_absolute_error, metrics.r2_score, metrics.roc_auc_score,\n",
    "                         metrics.average_precision_score]\n",
    "\n",
    "\n",
    "    column_names = [\"algorithm\", \"runtime_s\"] + [metric.__name__ for metric in competing_metrics] # + ['TN', 'FN', 'TP', 'FP', 'sensitivity', 'specificity', 'PPV', 'NPV']\n",
    "\n",
    "    results = []\n",
    "    results_val = []\n",
    "    for algorithm_name, algorithm in algorithms.items():\n",
    "\n",
    "        algorithm.fit(x_train, y_train)\n",
    "        \n",
    "        row, y_pred = evaluate(competing_metrics, algorithm_name, algorithm, x_test, y_test)\n",
    "        results.append(row)\n",
    "        # current_experiment.add_val_result(fold_idx=fold_idx, algorithm_name=algorithm_name, num_clients=0, split_method='central', name='internal_val', y_true=y_test, y_pred=y_pred)\n",
    "        current_experiment.add_to_kfold_table(algorithm_name=algorithm_name, num_clients=0, split_method='central', val_name='internal test', y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "        row, y_addit_pred = evaluate(competing_metrics, algorithm_name, algorithm, x_addit_test, y_addit_test)\n",
    "        results_val.append(row)\n",
    "        # current_experiment.add_val_result(fold_idx=fold_idx, algorithm_name=algorithm_name, num_clients=0, split_method='central', name='eternal_val', y_true=y_addit_test, y_pred=y_addit_pred)\n",
    "        current_experiment.add_to_kfold_table(algorithm_name=algorithm_name, num_clients=0, split_method='central', val_name='external test', y_true=y_addit_test, y_pred=y_addit_pred)\n",
    "        \n",
    "    res = process_results(column_names, results)\n",
    "    results_val = process_results(column_names, results_val)\n",
    "    \n",
    "    return res, results_val\n",
    "\n",
    "def get_split(dataset, splits):\n",
    "    indeces = list(range(0, len(dataset)))\n",
    "    np.random.shuffle(indeces)\n",
    "    subsets = []\n",
    "    for portion in splits:\n",
    "        offset = 0\n",
    "        if (subsets):\n",
    "            offset = len(subsets[-1])\n",
    "\n",
    "        indeces_partition = ceil(len(dataset) * portion)\n",
    "        subset = dataset[offset: min(offset + indeces_partition, len(dataset))]\n",
    "        subset = subset.reset_index(drop=True)\n",
    "        # print(f\"split {portion} - len: {len(subset)} actual: {len(subset) / len(dataset)}\")\n",
    "        # display(subset)\n",
    "        subsets.append(subset)\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "kfold_results = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "for fold_idx in range(current_experiment.K):\n",
    "    current_experiment.set_fold(fold_idx=fold_idx)\n",
    "\n",
    "    # get processed datasets\n",
    "    train = current_experiment.training_dataset\n",
    "    internal, external = current_experiment.get_combined_test_dataset()\n",
    "    test = internal[1]\n",
    "    addit_test = external[1]\n",
    "    \n",
    "    # separate predictors\n",
    "    x_train, y_train = current_experiment.as_features_labels(train, current_experiment.LABEL_COL)\n",
    "    x_test, y_test = current_experiment.as_features_labels(test, current_experiment.LABEL_COL)\n",
    "    x_external, y_external = current_experiment.as_features_labels(addit_test, current_experiment.LABEL_COL)\n",
    "    \n",
    "\n",
    "    result, result_val = compete(fold_idx, algorithms, x_train, y_train, x_test, y_test, x_external, y_external)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(current_experiment.kfold_table['algorithm_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'internal test'].groupby(current_experiment.metadata_column_names)\n",
    "display(internal_only.mean())\n",
    "exteral_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'external test'].groupby(current_experiment.metadata_column_names)\n",
    "display(exteral_only.mean())\n",
    "current_experiment.write_results('/Users/benjamindanek/Code/federated_learning_multi_modality_ancestry/multi_modality_fl/results/dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('/Users/benjamindanek/Code/federated_learning_multi_modality_ancestry/multi_modality_fl/results/dataframes/baselines.csv')[current_experiment.kfold_table['val_name'] == 'external_val'].groupby(current_experiment.metadata_column_names).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c1d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nih_fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
