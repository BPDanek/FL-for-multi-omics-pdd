{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from nvflare.apis.fl_constant import JobConstants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_modality_fl.utils.data_management import GlobalExperimentsConfiguration, write_json, read_json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset for Training/Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment = GlobalExperimentsConfiguration(\n",
    "    base_path=os.path.join(os.getcwd(), os.path.join('multi_modality_fl', 'experiments')),\n",
    "    experiment_name='federated_svm',\n",
    "    random_seed=0\n",
    ")\n",
    "\n",
    "current_experiment.create_experiment(\n",
    "    dataset_folder='/Users/benjamindanek/Code/federated_learning_multi_modality_ancestry/data',\n",
    "    dataset=GlobalExperimentsConfiguration.MULTIMODALITY,\n",
    "    split_method=GlobalExperimentsConfiguration.SKLEARN\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_KEY = \"server\"\n",
    "\"\"\"server FL site\"\"\"\n",
    "SERVER_VAL = f\"app_{SERVER_KEY}\"\n",
    "\"\"\"server FL app name\"\"\"\n",
    "\n",
    "def CLIENT_KEY(site_name_prefix, i):\n",
    "    \"\"\"client FL site\"\"\"\n",
    "    return f\"{site_name_prefix}{i}\"\n",
    "\n",
    "def CLIENT_VAL(site_name_prefix, i): \n",
    "    \"\"\"client FL app name\"\"\"\n",
    "    return f\"app_{site_name_prefix}{i}\"\n",
    "\n",
    "def get_deploy_map(site_name_prefix: str, n_sites: int):\n",
    "    \"\"\"\n",
    "    Generate a map of which apps in the job being uploaded will be deployed to which FL client sites.\n",
    "    \n",
    "    https://nvflare.readthedocs.io/en/main/real_world_fl/job.html#deploy-map\n",
    "    \"\"\"\n",
    "    deploy_map = {SERVER_VAL: [SERVER_KEY]}\n",
    "    for i in range(1, n_sites + 1):\n",
    "        deploy_map[CLIENT_VAL(site_name_prefix, i)] = [CLIENT_KEY(site_name_prefix, i)]\n",
    "\n",
    "    return deploy_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nvflare experiments as jobs\n",
    "ALL_JOBS_PATH = os.path.join(current_experiment.experiment_path, 'jobs')\n",
    "\"\"\"The portion of the experiment data path which is reserved for nvflare job definitions\"\"\"\n",
    "\n",
    "# root for this series of jobs\n",
    "# It is convenient to conduct several experiments at a time, so this interface was developed. \n",
    "JOB_BASE_FOLDER = 'svm_base'\n",
    "\"\"\"The root of all jobs for the current experiment. (ie `random_forest_base`)\"\"\"\n",
    "\n",
    "# Base folder for jobs\n",
    "base_path = os.path.join(ALL_JOBS_PATH, JOB_BASE_FOLDER)\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base job\n",
    "# we copy the base job when generating new jobs & change a few aspects in the design of experiemnts\n",
    "\n",
    "# 1. define meta\n",
    "base_job_meta_path = os.path.join(base_path, JobConstants.META_FILE)\n",
    "base_job_meta = {\n",
    "  \"name\": \"sklearn_svm\",\n",
    "  \"resource_spec\": {},\n",
    "  \"deploy_map\": {\n",
    "    \"app\": [\n",
    "      \"@ALL\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "# src from: https://github.com/NVIDIA/NVFlare/blob/main/examples/advanced/random_forest/jobs/random_forest_base/meta.json\n",
    "write_json(base_job_meta, base_job_meta_path)\n",
    "\n",
    "# 2. define server & client configs\n",
    "base_job_root = os.path.join(base_path, \"app\")\n",
    "base_job_configs = os.path.join(base_job_root, \"config\")\n",
    "if not os.path.exists(base_job_configs):\n",
    "    os.makedirs(base_job_configs, exist_ok=True)\n",
    "\n",
    "# 2.1. define base job config for server\n",
    "base_job_server_config_path = os.path.join(base_job_configs, JobConstants.SERVER_JOB_CONFIG)\n",
    "base_job_server_config = {\n",
    "  \"format_version\": 2,\n",
    "  \"min_clients\": 3,\n",
    "  \"num_rounds\": 2,\n",
    "  \"server\": {\n",
    "    \"heart_beat_timeout\": 600,\n",
    "    \"task_request_interval\": 0.05\n",
    "  },\n",
    "  \"task_data_filters\": [],\n",
    "  \"task_result_filters\": [],\n",
    "  \"components\": [\n",
    "    {\n",
    "      \"id\": \"persistor\",\n",
    "      \"path\": \"nvflare.app_opt.sklearn.joblib_model_param_persistor.JoblibModelParamPersistor\",\n",
    "      \"args\": {\n",
    "        \"initial_params\": {\n",
    "          \"kernel\": \"linear\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"shareable_generator\",\n",
    "      \"name\": \"FullModelShareableGenerator\",\n",
    "      \"args\": {}\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"aggregator\",\n",
    "      \"path\": \"nvflare.app_common.aggregators.collect_and_assemble_aggregator.CollectAndAssembleAggregator\",\n",
    "      \"args\": {\n",
    "        \"assembler_id\" : \"svm_assembler\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"svm_assembler\",\n",
    "      \"path\": \"svm_assembler.SVMAssembler\",\n",
    "      \"args\": {\n",
    "        \"kernel\": \"linear\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"workflows\": [\n",
    "    {\n",
    "      \"id\": \"scatter_and_gather\",\n",
    "      \"name\": \"ScatterAndGather\",\n",
    "      \"args\": {\n",
    "        \"min_clients\" : \"{min_clients}\",\n",
    "        \"num_rounds\" : \"{num_rounds}\",\n",
    "        \"start_round\": 0,\n",
    "        \"wait_time_after_min_received\": 0,\n",
    "        \"aggregator_id\": \"aggregator\",\n",
    "        \"persistor_id\": \"persistor\",\n",
    "        \"shareable_generator_id\": \"shareable_generator\",\n",
    "        \"train_task_name\": \"train\",\n",
    "        \"train_timeout\": 0\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "# src: https://github.com/NVIDIA/NVFlare/blob/main/examples/advanced/random_forest/jobs/random_forest_base/app/config/config_fed_server.json\n",
    "write_json(base_job_server_config, base_job_server_config_path)\n",
    "\n",
    "# 2.2. define the base job config for a client\n",
    "base_job_client_config_path = os.path.join(base_job_configs, JobConstants.CLIENT_JOB_CONFIG)\n",
    "base_job_client_config = {\n",
    "  \"format_version\": 2,\n",
    "\n",
    "  \"executors\": [\n",
    "    {\n",
    "      \"tasks\": [\"train\"],\n",
    "      \"executor\": {\n",
    "        \"id\": \"Executor\",\n",
    "        \"path\": \"nvflare.app_opt.sklearn.sklearn_executor.SKLearnExecutor\",\n",
    "        \"args\": {\n",
    "          \"learner_id\": \"svm_learner\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"task_result_filters\": [],\n",
    "  \"task_data_filters\": [],\n",
    "  \"components\": [\n",
    "    {\n",
    "      \"id\": \"svm_learner\",\n",
    "      \"path\": \"svm_learner.SVMLearner\",\n",
    "      \"args\": {\n",
    "        \"random_state\": current_experiment.RANDOM_SEED\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "# src: https://github.com/NVIDIA/NVFlare/blob/main/examples/advanced/xgboost/histogram-based/jobs/base/app/config/config_fed_client.json\n",
    "write_json(base_job_client_config, base_job_client_config_path)\n",
    "\n",
    "# 3. copy over custom contents for this experiment\n",
    "# source\n",
    "custom_data = os.path.join(os.getcwd(), \"multi_modality_fl\", \"models\", \"nvflare\", \"svm_custom\")\n",
    "\n",
    "# destination\n",
    "base_job_custom = os.path.join(base_job_root, \"custom\")\n",
    "os.makedirs(base_job_custom, exist_ok=True)\n",
    "# recursive copy\n",
    "shutil.copytree(custom_data, base_job_custom, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each job base folder has a meta file defining the topology of the federation for that particular job.\n",
    "# each job within the base folder shares the same meta file, but has different parameters specified.\n",
    "\n",
    "# SOME PARAMETERS TO PLAY WITH: https://github.com/NVIDIA/NVFlare/blob/e217679c4de035564a6ed9c2e2658197b0c8e701/examples/advanced/random_forest/utils/prepare_job_config.py#L35-L44\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "def prepare_nvflare_linear_experiment(\n",
    "        site_naming_fn: callable,\n",
    "        job_name: str, \n",
    "        site_prefix: str, \n",
    "        num_clients: int, \n",
    "        split_method: str, \n",
    "        site_config_naming_fn: Callable[..., Any], \n",
    "        local_subsample: str, \n",
    "        lr_mode: str\n",
    "    ) -> dict:\n",
    "\n",
    "    job = {\n",
    "        \"prefix\": site_prefix,\n",
    "        \"n_sites\": num_clients,\n",
    "        \"split_method\": split_method,\n",
    "        \"local_subsample\": local_subsample, # hyper-parameter https://www.r-bloggers.com/2021/08/feature-subsampling-for-random-forest-regression/\n",
    "        \"lr_scale\": num_clients, # investigate this param: https://github.com/NVIDIA/NVFlare/blob/e217679c4de035564a6ed9c2e2658197b0c8e701/examples/advanced/random_forest/utils/prepare_job_config.py#L93\n",
    "        \"lr_mode\": lr_mode, # or \"scaled\"\n",
    "        \"nthread\": 16,\n",
    "        # \"min_clients\" : 0,\n",
    "        \"num_rounds\" : 1,\n",
    "        \"start_round\": 0,\n",
    "        \"wait_time_after_min_received\": 0,\n",
    "        \"aggregator_id\": \"aggregator\",\n",
    "        \"persistor_id\": \"persistor\",\n",
    "        \"shareable_generator_id\": \"shareable_generator\",\n",
    "        \"train_task_name\": \"train\",\n",
    "        \"train_timeout\": 0,\n",
    "        \"_name\": job_name\n",
    "    }\n",
    "\n",
    "    # make the folder for the job\n",
    "    path = os.path.join(ALL_JOBS_PATH, job_name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True) \n",
    "    job[\"_path\"] = path\n",
    "\n",
    "    # 1. define the meta file for this job by augmenting the base file and writing it to a new job folder\n",
    "    meta_config = read_json(base_job_meta_path)\n",
    "    meta_config[\"name\"] = job_name\n",
    "    meta_config[\"deploy_map\"] = get_deploy_map(job[\"prefix\"], job[\"n_sites\"])\n",
    "    meta_config[\"min_clients\"] = job[\"n_sites\"]\n",
    "    write_json(meta_config, os.path.join(path, JobConstants.META_FILE))\n",
    "\n",
    "\n",
    "    # 2. define the server & client configs for this job by augmenting the base file and writing it to a new job folder\n",
    "    # 2.1.1 make job server config\n",
    "    server_config = read_json(base_job_server_config_path)\n",
    "    server_config[\"min_clients\"] = job[\"n_sites\"]\n",
    "    server_config[\"workflows\"][0][\"args\"][\"min_clients\"] = job[\"n_sites\"]\n",
    "    server_config[\"workflows\"][0][\"args\"][\"num_rounds\"] = 1 # svm\n",
    "\n",
    "    server_app_name = SERVER_VAL\n",
    "    server_config_path = os.path.join(path, server_app_name, \"config\")\n",
    "    if not os.path.exists(server_config_path):\n",
    "        os.makedirs(server_config_path, exist_ok=True)\n",
    "    write_json(server_config, os.path.join(server_config_path, JobConstants.SERVER_JOB_CONFIG))\n",
    "\n",
    "    # 2.2 make job client config\n",
    "    for site_idx in range(job[\"n_sites\"]):\n",
    "        \n",
    "        client_app_name = CLIENT_VAL(job[\"prefix\"], site_idx + 1)\n",
    "        client_path = os.path.join(path, client_app_name)\n",
    "        if not os.path.exists(client_path):\n",
    "            os.makedirs(client_path, exist_ok=True)\n",
    "\n",
    "        client_config_path = os.path.join(client_path, \"config\")\n",
    "        if not os.path.exists(client_config_path):\n",
    "            os.makedirs(client_config_path, exist_ok=True)\n",
    "\n",
    "        # 2.2.1 update client config\n",
    "        client_config = read_json(base_job_client_config_path)\n",
    "       \n",
    "        # path for json which defines site split\n",
    "        data_split_name = os.path.join(current_experiment.experiment_path, site_config_naming_fn(site_idx))\n",
    "        \n",
    "        client_config[\"components\"][0][\"args\"][\"data_split_filename\"] = data_split_name\n",
    "        client_config[\"components\"][0][\"args\"][\"client_id\"] = site_naming_fn(site_idx)\n",
    "        \n",
    "        # https://github.com/NVIDIA/NVFlare/blob/1433290c203bd23f34c29e11795ce592bc067888/examples/advanced/sklearn-linear/utils/prepare_job_config.py#L190C5-L194C80\n",
    "        write_json(client_config, os.path.join(client_config_path, JobConstants.CLIENT_JOB_CONFIG))\n",
    "\n",
    "        # 2.2.2 copy over client custom files\n",
    "        client_custom_path = os.path.join(client_path, \"custom\")\n",
    "        if not os.path.exists(client_custom_path):\n",
    "            os.makedirs(client_custom_path, exist_ok=True)\n",
    "        shutil.copytree(base_job_custom, client_custom_path, dirs_exist_ok=True)\n",
    "\n",
    "    return job\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare.private.fed.app.simulator.simulator_runner import SimulatorRunner\n",
    "from joblib import load\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for fold_idx in range(current_experiment.K):\n",
    "    current_experiment.set_fold(fold_idx=fold_idx)\n",
    "    current_experiment.set_validation_dataset()\n",
    "        \n",
    "    # save normalized dataset\n",
    "    # training_full_processed_path = os.path.join(current_experiment.experiment_path, 'training_processed.h5')\n",
    "    # testing_full_processed_path = os.path.join(current_experiment.experiment_path, 'test_processed.h5')\n",
    "    # validation_ful_processed_path = os.path.join(current_experiment.experiment_path, 'validation_processed.h5')\n",
    "\n",
    "    # current_experiment.training_dataset.to_hdf(training_full_processed_path, key='df', mode='w')\n",
    "    # current_experiment.test_dataset.to_hdf(testing_full_processed_path, key='df', mode='w')\n",
    "    # current_experiment.get_internal_val().to_hdf(validation_ful_processed_path, key='df', mode='w')\n",
    "\n",
    "\n",
    "    # generate data for several site configurations\n",
    "    # each configuration is a json, so there is no duplication of underlying data\n",
    "    site_configs = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    # site_configs = [2, 3, 4, 5, 6]\n",
    "    site_prefixes = [\"site-\"] * len(site_configs)\n",
    "    split_methods = [\"uniform\"] * len(site_configs)\n",
    "\n",
    "    for i in range(len(site_configs)):\n",
    "        num_clients, site_prefix, split_method = site_configs[i], site_prefixes[i], split_methods[i]\n",
    "        print(len(current_experiment.training_dataset))\n",
    "        # 1. split the data for the experiment\n",
    "        client_dataframes = current_experiment.get_stratified_client_subsets(\n",
    "            dataset=current_experiment.training_dataset, \n",
    "            num_clients=num_clients,\n",
    "            method=split_method\n",
    "        )\n",
    "\n",
    "        # translate data frames into client splits & write the data\n",
    "        client_splits = []\n",
    "        training_data_paths = []\n",
    "        validaiton_data_paths = []\n",
    "        for i, df in enumerate(client_dataframes):\n",
    "            start, end = 0, len(df)\n",
    "            client_splits.append((start, end))\n",
    "            \n",
    "            training_subset_path = os.path.join(current_experiment.experiment_path, f'client_training_{i}_stratified.h5')\n",
    "            df.to_hdf(training_subset_path, key='df', mode='w')\n",
    "            training_data_paths.append(training_subset_path)\n",
    "\n",
    "            validation_subset_path = os.path.join(current_experiment.experiment_path, f'client_validation_{i}_stratified.h5')\n",
    "            df.to_hdf(validation_subset_path, key='df', mode='w')\n",
    "            validaiton_data_paths.append(validation_subset_path)\n",
    "\n",
    "        def site_naming_fn(site_index):\n",
    "            \"\"\"Used for naming files in the client data split json\"\"\"\n",
    "            return f\"{site_prefix}{site_index + 1}\"\n",
    "\n",
    "        def to_subset_id(site_index: int):\n",
    "            \"\"\"Name client data subsets in a human readable fashion. site_number is 1 indexed\"\"\"\n",
    "            return f'{split_method}_sid_{site_index}_of_{num_clients}.json'\n",
    "        \n",
    "        filenames, client_jsons = current_experiment.nvflare_multi_site_split_json(\n",
    "            data_source_path=training_data_paths, \n",
    "            validation_data_source_path=validaiton_data_paths,\n",
    "            client_splits=client_splits,\n",
    "            site_naming_fn=site_naming_fn,\n",
    "            site_config_naming_fn=to_subset_id,\n",
    "        )\n",
    "\n",
    "        for filename, client_json in zip(filenames, client_jsons):\n",
    "            print(filename, client_json)\n",
    "            write_json(client_json, os.path.join(current_experiment.experiment_path, filename))\n",
    "\n",
    "\n",
    "        # 2. define the simulation job\n",
    "\n",
    "        # hyper-parem defaults from tutorial\n",
    "        local_subsample=1\n",
    "        lr_mode=\"uniform\"\n",
    "\n",
    "        def get_job_name(local_subsample, lr_mode):\n",
    "            \"\"\"\n",
    "            The unique id for this experiment in the context of NVFlare\n",
    "            Args:\n",
    "                local_subsample: Local random forest subsample rate https://github.com/NVIDIA/NVFlare/blame/e217679c4de035564a6ed9c2e2658197b0c8e701/examples/advanced/random_forest/utils/prepare_job_config.py#L38\n",
    "                lr_mode: Whether to use uniform or scaled shrinkage\n",
    "            \"\"\"\n",
    "            local_subsample = int(local_subsample * 100)\n",
    "            return f\"svm_{site_prefix}_{num_clients}_sites_{local_subsample}_ls_{split_method}_sm_{lr_mode}_lr\"\n",
    "\n",
    "        job_name = get_job_name(local_subsample, lr_mode)\n",
    "\n",
    "        job_config = prepare_nvflare_linear_experiment(\n",
    "            site_naming_fn=site_naming_fn,\n",
    "            job_name=job_name,\n",
    "            site_prefix=site_prefix,\n",
    "            num_clients=num_clients,\n",
    "            split_method=split_method,\n",
    "            site_config_naming_fn=to_subset_id, # used for getting the config file for the sites\n",
    "            local_subsample=local_subsample,\n",
    "            lr_mode=lr_mode\n",
    "        )\n",
    "\n",
    "        workspace_path = f\"/tmp/nvflare/workspaces/{job_name}\"\n",
    "        print(\"workspace_path:\", workspace_path)\n",
    "        print(job_config[\"_path\"])\n",
    "        \n",
    "        simulator = SimulatorRunner(\n",
    "            job_folder=job_config[\"_path\"],\n",
    "            workspace=workspace_path,\n",
    "            n_clients=job_config[\"n_sites\"],\n",
    "            threads=job_config[\"n_sites\"]\n",
    "        )\n",
    "        run_status = simulator.run()\n",
    "        print(\"Simulator finished with run_status\", run_status)\n",
    "\n",
    "\n",
    "        # validate simulation\n",
    "        args = {}\n",
    "        model_path = os.path.join(workspace_path, \"simulate_job\", SERVER_VAL, \"model_param.joblib\")\n",
    "        num_trees = 100 # hyper-param?\n",
    "        param = {}\n",
    "        param[\"objective\"] = \"binary:logistic\"\n",
    "        param[\"eta\"] = 0.1\n",
    "        param[\"max_depth\"] = 8\n",
    "        param[\"eval_metric\"] = \"auc\"\n",
    "        param[\"nthread\"] = 16\n",
    "        param[\"num_parallel_tree\"] = num_trees\n",
    "\n",
    "        model_params = load(model_path)\n",
    "\n",
    "        svm_global = SVC(kernel='rbf')\n",
    "        support_x = model_params[\"support_x\"]\n",
    "        support_y = model_params[\"support_y\"]\n",
    "        \n",
    "        svm_global.fit(support_x, support_y)\n",
    "\n",
    "        validation_results = []\n",
    "        for name, dataset in current_experiment.get_combined_test_dataset():\n",
    "            \n",
    "            X, y = current_experiment.as_features_labels(dataset, current_experiment.LABEL_COL)\n",
    "\n",
    "            # validate model performance\n",
    "            y_pred = svm_global.predict(X)\n",
    "            print(y_pred.dtype)\n",
    "            y_pred = y_pred.astype(int)\n",
    "\n",
    "            current_experiment.add_to_kfold_table(\n",
    "                fold_number=fold_idx,\n",
    "                algorithm_name='Federated SVClassifier', \n",
    "                num_clients=num_clients, \n",
    "                split_method=split_method,\n",
    "                val_name=name,\n",
    "                y_true=y, \n",
    "                y_pred=y_pred\n",
    "            )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'internal test'].groupby(current_experiment.metadata_column_names)\n",
    "display(internal_only.std())\n",
    "exteral_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'external test'].groupby(current_experiment.metadata_column_names)\n",
    "display(exteral_only.std())\n",
    "current_experiment.write_results('/Users/benjamindanek/Code/federated_learning_multi_modality_ancestry/multi_modality_fl/results/dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'internal_validation'].groupby(current_experiment.metadata_column_names)\n",
    "display(internal_only.mean())\n",
    "exteral_only = current_experiment.kfold_table[current_experiment.kfold_table['val_name'] == 'external_validation'].groupby(current_experiment.metadata_column_names)\n",
    "display(exteral_only.mean())\n",
    "current_experiment.write_results('/Users/benjamindanek/Code/federated_learning_multi_modality_ancestry/multi_modality_fl/results/dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment.kfold_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment.write_results('/Users/benjamindanek/Code/federated_learning_multi_modality_ancestry/multi_modality_fl/results/dataframes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nih_fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
